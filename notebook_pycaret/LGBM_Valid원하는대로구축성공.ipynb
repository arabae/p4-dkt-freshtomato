{"cells":[{"cell_type":"markdown","metadata":{"id":"ZipXHWdgaQz-"},"source":["https://minimin2.tistory.com/137\n","\n","https://leo-bb.tistory.com/62?category=858291\n","\n","https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.std.html"]},{"cell_type":"markdown","metadata":{},"source":["# 결측치 많은 변수들\n","\n","'user_test_correct_cnt_per_test': 372603,\n","\n","'user_acc_per_test': 372603,\n","\n","'user_total_acc': 365164,\n","\n"," 'user_total_correct_cnt': 365164"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pycaret[full] in /opt/conda/lib/python3.7/site-packages (2.3.1)\n","Requirement already satisfied: ngboost in /opt/conda/lib/python3.7/site-packages (0.3.10)\n","Collecting shap\n","  Using cached shap-0.39.0.tar.gz (356 kB)\n","Requirement already satisfied: wordcloud in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.8.1)\n","Requirement already satisfied: yellowbrick>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.3.post1)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.0.1)\n","Requirement already satisfied: textblob in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.15.3)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (3.4.2)\n","Requirement already satisfied: scikit-plot in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.3.7)\n","Requirement already satisfied: umap-learn in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.5.1)\n","Requirement already satisfied: IPython in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (7.16.1)\n","Requirement already satisfied: pandas-profiling>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (3.0.0)\n","Requirement already satisfied: imbalanced-learn==0.7.0 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.7.0)\n","Requirement already satisfied: Boruta in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.3)\n","Requirement already satisfied: kmodes>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.11.0)\n","Requirement already satisfied: numpy==1.19.5 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.19.5)\n","Requirement already satisfied: mlxtend>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.18.0)\n","Requirement already satisfied: pyLDAvis in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (3.3.1)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.2.4)\n","Requirement already satisfied: scipy<=1.5.4 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.5.4)\n","Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (7.6.3)\n","Requirement already satisfied: pyod in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.8.8)\n","Requirement already satisfied: spacy<2.4.0 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (2.3.5)\n","Requirement already satisfied: lightgbm>=2.3.1 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (3.2.1)\n","Requirement already satisfied: plotly>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (4.14.3)\n","Requirement already satisfied: gensim<4.0.0 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (3.8.3)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (3.6.2)\n","Requirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.11.1)\n","Requirement already satisfied: mlflow in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.17.0)\n","Requirement already satisfied: cufflinks>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.17.3)\n","Requirement already satisfied: scikit-learn==0.23.2 in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.23.2)\n","Requirement already satisfied: psutil; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (5.7.0)\n","Requirement already satisfied: awscli; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.19.85)\n","Requirement already satisfied: catboost>=0.23.2; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.25.1)\n","Requirement already satisfied: xgboost>=1.1.0; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.4.2)\n","Requirement already satisfied: scikit-optimize>=0.8.1; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.8.1)\n","Requirement already satisfied: azure-storage-blob; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (12.8.1)\n","Requirement already satisfied: hyperopt; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.2.5)\n","Requirement already satisfied: tune-sklearn>=0.2.1; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.3.0)\n","Requirement already satisfied: ray[tune]>=1.0.0; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.3.0)\n","Requirement already satisfied: optuna; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (0.18.1)\n","Requirement already satisfied: google-cloud-storage; extra == \"full\" in /opt/conda/lib/python3.7/site-packages (from pycaret[full]) (1.38.0)\n","Requirement already satisfied: importlib-metadata<4.0.0,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from ngboost) (3.10.1)\n","Requirement already satisfied: lifelines<0.29,>=0.25 in /opt/conda/lib/python3.7/site-packages (from ngboost) (0.26.0)\n","Requirement already satisfied: tqdm<5.0,>=4.4 in /opt/conda/lib/python3.7/site-packages (from ngboost) (4.61.0)\n","Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.7/site-packages (from shap) (0.0.7)\n","Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from shap) (0.53.1)\n","Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from shap) (1.6.0)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from wordcloud->pycaret[full]) (7.2.0)\n","Requirement already satisfied: cycler>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from yellowbrick>=1.0.1->pycaret[full]) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret[full]) (1.3.1)\n","Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret[full]) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret[full]) (2.8.1)\n","Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.7/site-packages (from umap-learn->pycaret[full]) (0.5.2)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (46.4.0.post20200518)\n","Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (0.17.1)\n","Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (4.3.3)\n","Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (0.2.0)\n","Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (4.4.2)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (3.0.5)\n","Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret[full]) (2.6.1)\n","Requirement already satisfied: jinja2>=2.11.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (2.11.2)\n","Requirement already satisfied: pydantic>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (1.8.2)\n","Requirement already satisfied: visions[type_image_path]==0.7.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.7.1)\n","Requirement already satisfied: tangled-up-in-unicode==0.1.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.1.0)\n","Requirement already satisfied: phik>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.11.2)\n","Requirement already satisfied: PyYAML>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (5.3.1)\n","Requirement already satisfied: htmlmin>=0.1.12 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.1.12)\n","Requirement already satisfied: requests>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (2.25.1)\n","Requirement already satisfied: missingno>=0.4.2 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret[full]) (0.4.2)\n","Requirement already satisfied: numexpr in /opt/conda/lib/python3.7/site-packages (from pyLDAvis->pycaret[full]) (2.7.3)\n","Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyLDAvis->pycaret[full]) (0.18.2)\n","Requirement already satisfied: funcy in /opt/conda/lib/python3.7/site-packages (from pyLDAvis->pycaret[full]) (1.16)\n","Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (from pyLDAvis->pycaret[full]) (0.0)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->pycaret[full]) (2020.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret[full]) (1.0.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret[full]) (5.5.5)\n","Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret[full]) (5.1.3)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret[full]) (3.5.1)\n","Requirement already satisfied: statsmodels in /opt/conda/lib/python3.7/site-packages (from pyod->pycaret[full]) (0.12.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from pyod->pycaret[full]) (1.14.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (1.0.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (0.7.4)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (7.4.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (0.8.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (3.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (2.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (1.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret[full]) (1.0.5)\n","Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm>=2.3.1->pycaret[full]) (0.34.2)\n","Requirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from plotly>=4.4.1->pycaret[full]) (1.3.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0.0->pycaret[full]) (5.1.0)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk->pycaret[full]) (2021.4.4)\n","Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->pycaret[full]) (8.0.1)\n","Requirement already satisfied: prometheus-flask-exporter in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (0.18.2)\n","Requirement already satisfied: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (3.1.17)\n","Requirement already satisfied: querystring-parser in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (1.2.4)\n","Requirement already satisfied: alembic<=1.4.1 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (1.4.1)\n","Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (1.4.15)\n","Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (3.17.0)\n","Requirement already satisfied: databricks-cli>=0.8.7 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (0.14.3)\n","Requirement already satisfied: gunicorn; platform_system != \"Windows\" in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (20.1.0)\n","Requirement already satisfied: sqlparse>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (0.4.1)\n","Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (0.3)\n","Requirement already satisfied: Flask in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (2.0.1)\n","Requirement already satisfied: docker>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret[full]) (5.0.0)\n","Requirement already satisfied: colorlover>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from cufflinks>=0.17.0->pycaret[full]) (0.3.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2->pycaret[full]) (2.1.0)\n","Requirement already satisfied: rsa<4.8,>=3.1.2; python_version > \"2.7\" in /opt/conda/lib/python3.7/site-packages (from awscli; extra == \"full\"->pycaret[full]) (4.7.2)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from awscli; extra == \"full\"->pycaret[full]) (0.15.2)\n","Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from awscli; extra == \"full\"->pycaret[full]) (0.4.3)\n","Requirement already satisfied: botocore==1.20.85 in /opt/conda/lib/python3.7/site-packages (from awscli; extra == \"full\"->pycaret[full]) (1.20.85)\n","Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from awscli; extra == \"full\"->pycaret[full]) (0.4.2)\n","Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from catboost>=0.23.2; extra == \"full\"->pycaret[full]) (0.16)\n","Requirement already satisfied: pyaml>=16.9 in /opt/conda/lib/python3.7/site-packages (from scikit-optimize>=0.8.1; extra == \"full\"->pycaret[full]) (20.4.0)\n","Requirement already satisfied: azure-core<2.0.0,>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from azure-storage-blob; extra == \"full\"->pycaret[full]) (1.14.0)\n","Requirement already satisfied: msrest>=0.6.18 in /opt/conda/lib/python3.7/site-packages (from azure-storage-blob; extra == \"full\"->pycaret[full]) (0.6.21)\n","Requirement already satisfied: cryptography>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from azure-storage-blob; extra == \"full\"->pycaret[full]) (2.9.2)\n","Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from hyperopt; extra == \"full\"->pycaret[full]) (2.5.1)\n","Requirement already satisfied: py-spy>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (0.3.7)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (3.0.12)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (1.0.2)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (3.7.4.post0)\n","Requirement already satisfied: aioredis in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (1.3.1)\n","Requirement already satisfied: prometheus-client>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (0.10.1)\n","Requirement already satisfied: gpustat in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (0.6.0)\n","Requirement already satisfied: aiohttp-cors in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (0.7.0)\n","Requirement already satisfied: opencensus in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (0.7.13)\n","Requirement already satisfied: grpcio>=1.28.1 in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (1.38.0)\n","Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (3.2.0)\n","Requirement already satisfied: redis>=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (3.5.3)\n","Requirement already satisfied: tensorboardX; extra == \"tune\" in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (2.2)\n","Requirement already satisfied: tabulate; extra == \"tune\" in /opt/conda/lib/python3.7/site-packages (from ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (0.8.9)\n","Requirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna; extra == \"full\"->pycaret[full]) (5.0.1)\n","Requirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna; extra == \"full\"->pycaret[full]) (3.7.0)\n","Requirement already satisfied: typing in /opt/conda/lib/python3.7/site-packages (from optuna; extra == \"full\"->pycaret[full]) (3.7.4.3)\n","Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage; extra == \"full\"->pycaret[full]) (1.6.0)\n","Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage; extra == \"full\"->pycaret[full]) (1.30.1)\n","Requirement already satisfied: google-resumable-media<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage; extra == \"full\"->pycaret[full]) (1.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<4.0.0,>=3.4.0->ngboost) (3.10.0.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<4.0.0,>=3.4.0->ngboost) (3.4.1)\n","Requirement already satisfied: autograd>=1.3 in /opt/conda/lib/python3.7/site-packages (from lifelines<0.29,>=0.25->ngboost) (1.3)\n","Requirement already satisfied: formulaic<0.3,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from lifelines<0.29,>=0.25->ngboost) (0.2.3)\n","Requirement already satisfied: autograd-gamma>=0.3 in /opt/conda/lib/python3.7/site-packages (from lifelines<0.29,>=0.25->ngboost) (0.5.0)\n","Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba->shap) (0.36.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->IPython->pycaret[full]) (0.6.0)\n","Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->IPython->pycaret[full]) (0.7.0)\n","Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->IPython->pycaret[full]) (0.2.0)\n","Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->pycaret[full]) (0.2.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2>=2.11.1->pandas-profiling>=2.8.0->pycaret[full]) (1.1.1)\n","Requirement already satisfied: bottleneck in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret[full]) (1.3.2)\n","Requirement already satisfied: multimethod==1.4 in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret[full]) (1.4)\n","Requirement already satisfied: attrs>=19.3.0 in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret[full]) (21.2.0)\n","Requirement already satisfied: imagehash; extra == \"type_image_path\" in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret[full]) (4.2.0)\n","Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret[full]) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret[full]) (2020.6.20)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret[full]) (1.25.8)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret[full]) (3.0.4)\n","Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->pycaret[full]) (6.1.12)\n","Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->pycaret[full]) (6.1)\n","Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->pycaret[full]) (4.7.1)\n","Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (6.4.0)\n","Requirement already satisfied: patsy>=0.5 in /opt/conda/lib/python3.7/site-packages (from statsmodels->pyod->pycaret[full]) (0.5.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow->pycaret[full]) (4.0.7)\n","Requirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic<=1.4.1->mlflow->pycaret[full]) (1.0.4)\n","Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic<=1.4.1->mlflow->pycaret[full]) (1.1.4)\n","Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from sqlalchemy->mlflow->pycaret[full]) (1.1.0)\n","Requirement already satisfied: Werkzeug>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow->pycaret[full]) (2.0.1)\n","Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow->pycaret[full]) (2.0.1)\n","Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=4.0.0->mlflow->pycaret[full]) (1.0.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.8,>=3.1.2; python_version > \"2.7\"->awscli; extra == \"full\"->pycaret[full]) (0.4.8)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore==1.20.85->awscli; extra == \"full\"->pycaret[full]) (0.10.0)\n","Requirement already satisfied: isodate>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from msrest>=0.6.18->azure-storage-blob; extra == \"full\"->pycaret[full]) (0.6.0)\n","Requirement already satisfied: requests-oauthlib>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from msrest>=0.6.18->azure-storage-blob; extra == \"full\"->pycaret[full]) (1.3.0)\n","Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.1.4->azure-storage-blob; extra == \"full\"->pycaret[full]) (1.14.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (5.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (1.6.3)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (3.0.1)\n","Requirement already satisfied: hiredis in /opt/conda/lib/python3.7/site-packages (from aioredis->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (2.0.0)\n","Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.7/site-packages (from gpustat->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (7.352.0)\n","Requirement already satisfied: blessings>=1.6 in /opt/conda/lib/python3.7/site-packages (from gpustat->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (1.7)\n","Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from opencensus->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (1.28.0)\n","Requirement already satisfied: opencensus-context==0.1.2 in /opt/conda/lib/python3.7/site-packages (from opencensus->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (0.1.2)\n","Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (0.17.3)\n","Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna; extra == \"full\"->pycaret[full]) (5.6.0)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna; extra == \"full\"->pycaret[full]) (2.1.0)\n","Requirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna; extra == \"full\"->pycaret[full]) (1.5.0)\n","Requirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna; extra == \"full\"->pycaret[full]) (3.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage; extra == \"full\"->pycaret[full]) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage; extra == \"full\"->pycaret[full]) (4.2.2)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage; extra == \"full\"->pycaret[full]) (1.1.2)\n","Requirement already satisfied: interface-meta>=1.2 in /opt/conda/lib/python3.7/site-packages (from formulaic<0.3,>=0.2.2->lifelines<0.29,>=0.25->ngboost) (1.2.3)\n","Requirement already satisfied: astor in /opt/conda/lib/python3.7/site-packages (from formulaic<0.3,>=0.2.2->lifelines<0.29,>=0.25->ngboost) (0.8.1)\n","Requirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from formulaic<0.3,>=0.2.2->lifelines<0.29,>=0.25->ngboost) (1.12.1)\n","Requirement already satisfied: PyWavelets in /opt/conda/lib/python3.7/site-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret[full]) (1.1.1)\n","Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->pycaret[full]) (22.0.3)\n","Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (1.5.0)\n","Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.10.0)\n","Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (6.0.7)\n","Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (20.1.0)\n","Requirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow->pycaret[full]) (4.0.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob; extra == \"full\"->pycaret[full]) (3.1.1)\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob; extra == \"full\"->pycaret[full]) (2.20)\n","Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (20.9)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]>=1.0.0; extra == \"full\"->pycaret[full]) (1.53.0)\n","Requirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna; extra == \"full\"->pycaret[full]) (1.8.2)\n","Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.1.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (1.4.3)\n","Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.5.0)\n","Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.5.3)\n","Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.7.1)\n","Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (3.3.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.8.4)\n","Requirement already satisfied: async-generator in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (1.10)\n","Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (1.5.1)\n","Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret[full]) (0.5.1)\n","Building wheels for collected packages: shap\n","  Building wheel for shap (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Command errored out with exit status 1:\n","   command: /opt/conda/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-e5dwqmo4/shap/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-e5dwqmo4/shap/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-0krri7ov\n","       cwd: /tmp/pip-install-e5dwqmo4/shap/\n","  Complete output (119 lines):\n","  The nvcc binary could not be located in your $PATH. Either  add it to your path, or set $CUDAHOME to enable CUDA\n","  Error building cuda module: TypeError('cannot unpack non-iterable NoneType object')\n","  WARNING: Could not compile cuda extensions\n","  running bdist_wheel\n","  running build\n","  running build_py\n","  creating build\n","  creating build/lib.linux-x86_64-3.7\n","  creating build/lib.linux-x86_64-3.7/shap\n","  copying shap/__init__.py -> build/lib.linux-x86_64-3.7/shap\n","  copying shap/_serializable.py -> build/lib.linux-x86_64-3.7/shap\n","  copying shap/links.py -> build/lib.linux-x86_64-3.7/shap\n","  copying shap/_explanation.py -> build/lib.linux-x86_64-3.7/shap\n","  copying shap/datasets.py -> build/lib.linux-x86_64-3.7/shap\n","  creating build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_explainer.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_partition.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_exact.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_linear.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/__init__.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_additive.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_permutation.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_gpu_tree.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_sampling.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_kernel.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/pytree.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_tree.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/_gradient.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/mimic.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  copying shap/explainers/tf_utils.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","  creating build/lib.linux-x86_64-3.7/shap/explainers/other\n","  copying shap/explainers/other/_maple.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","  copying shap/explainers/other/_treegain.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","  copying shap/explainers/other/__init__.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","  copying shap/explainers/other/_random.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","  copying shap/explainers/other/_lime.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","  copying shap/explainers/other/_coefficent.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","  creating build/lib.linux-x86_64-3.7/shap/explainers/_deep\n","  copying shap/explainers/_deep/deep_tf.py -> build/lib.linux-x86_64-3.7/shap/explainers/_deep\n","  copying shap/explainers/_deep/__init__.py -> build/lib.linux-x86_64-3.7/shap/explainers/_deep\n","  copying shap/explainers/_deep/deep_pytorch.py -> build/lib.linux-x86_64-3.7/shap/explainers/_deep\n","  creating build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_partial_dependence.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_bar.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_labels.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_utils.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_decision.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_monitoring.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_text.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_image.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_beeswarm.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_force_matplotlib.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_heatmap.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/__init__.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_scatter.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_force.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_violin.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_group_difference.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_embedding.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  copying shap/plots/_waterfall.py -> build/lib.linux-x86_64-3.7/shap/plots\n","  creating build/lib.linux-x86_64-3.7/shap/plots/colors\n","  copying shap/plots/colors/__init__.py -> build/lib.linux-x86_64-3.7/shap/plots/colors\n","  copying shap/plots/colors/_colorconv.py -> build/lib.linux-x86_64-3.7/shap/plots/colors\n","  copying shap/plots/colors/_colors.py -> build/lib.linux-x86_64-3.7/shap/plots/colors\n","  creating build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/experiments.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/methods.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/__init__.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/perturbation.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/framework.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/metrics.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/plots.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/measures.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  copying shap/benchmark/models.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","  creating build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/_tabular.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/_masker.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/_text.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/_image.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/__init__.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/_fixed.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/_fixed_composite.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/_output_composite.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  copying shap/maskers/_composite.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","  creating build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/_keras.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/_legacy.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/_general.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/__init__.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/_masked_model.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/transformers.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/_clustering.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/image.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  copying shap/utils/_show_progress.py -> build/lib.linux-x86_64-3.7/shap/utils\n","  creating build/lib.linux-x86_64-3.7/shap/actions\n","  copying shap/actions/_optimizer.py -> build/lib.linux-x86_64-3.7/shap/actions\n","  copying shap/actions/_action.py -> build/lib.linux-x86_64-3.7/shap/actions\n","  copying shap/actions/__init__.py -> build/lib.linux-x86_64-3.7/shap/actions\n","  creating build/lib.linux-x86_64-3.7/shap/models\n","  copying shap/models/_model.py -> build/lib.linux-x86_64-3.7/shap/models\n","  copying shap/models/_text_generation.py -> build/lib.linux-x86_64-3.7/shap/models\n","  copying shap/models/__init__.py -> build/lib.linux-x86_64-3.7/shap/models\n","  copying shap/models/_topk_lm.py -> build/lib.linux-x86_64-3.7/shap/models\n","  copying shap/models/_teacher_forcing.py -> build/lib.linux-x86_64-3.7/shap/models\n","  copying shap/models/_transformers_pipeline.py -> build/lib.linux-x86_64-3.7/shap/models\n","  creating build/lib.linux-x86_64-3.7/shap/plots/resources\n","  copying shap/plots/resources/bundle.js -> build/lib.linux-x86_64-3.7/shap/plots/resources\n","  copying shap/plots/resources/logoSmallGray.png -> build/lib.linux-x86_64-3.7/shap/plots/resources\n","  creating build/lib.linux-x86_64-3.7/shap/cext\n","  copying shap/cext/tree_shap.h -> build/lib.linux-x86_64-3.7/shap/cext\n","  running build_ext\n","  numpy.get_include() /opt/conda/lib/python3.7/site-packages/numpy/core/include\n","  building 'shap._cext' extension\n","  creating build/temp.linux-x86_64-3.7\n","  creating build/temp.linux-x86_64-3.7/shap\n","  creating build/temp.linux-x86_64-3.7/shap/cext\n","  gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.7m -I/opt/conda/lib/python3.7/site-packages/numpy/core/include -c shap/cext/_cext.cc -o build/temp.linux-x86_64-3.7/shap/cext/_cext.o\n","  gcc: error trying to exec 'cc1plus': execvp: No such file or directory\n","  error: command 'gcc' failed with exit status 1\n","  ----------------------------------------\u001b[0m\n","\u001b[31m  ERROR: Failed building wheel for shap\u001b[0m\n","\u001b[?25h  Running setup.py clean for shap\n","Failed to build shap\n","Installing collected packages: shap\n","    Running setup.py install for shap ... \u001b[?25lerror\n","\u001b[31m    ERROR: Command errored out with exit status 1:\n","     command: /opt/conda/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-e5dwqmo4/shap/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-e5dwqmo4/shap/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-k4dyove2/install-record.txt --single-version-externally-managed --compile --install-headers /opt/conda/include/python3.7m/shap\n","         cwd: /tmp/pip-install-e5dwqmo4/shap/\n","    Complete output (119 lines):\n","    The nvcc binary could not be located in your $PATH. Either  add it to your path, or set $CUDAHOME to enable CUDA\n","    Error building cuda module: TypeError('cannot unpack non-iterable NoneType object')\n","    WARNING: Could not compile cuda extensions\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.7\n","    creating build/lib.linux-x86_64-3.7/shap\n","    copying shap/__init__.py -> build/lib.linux-x86_64-3.7/shap\n","    copying shap/_serializable.py -> build/lib.linux-x86_64-3.7/shap\n","    copying shap/links.py -> build/lib.linux-x86_64-3.7/shap\n","    copying shap/_explanation.py -> build/lib.linux-x86_64-3.7/shap\n","    copying shap/datasets.py -> build/lib.linux-x86_64-3.7/shap\n","    creating build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_explainer.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_partition.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_exact.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_linear.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/__init__.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_additive.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_permutation.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_gpu_tree.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_sampling.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_kernel.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/pytree.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_tree.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/_gradient.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/mimic.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    copying shap/explainers/tf_utils.py -> build/lib.linux-x86_64-3.7/shap/explainers\n","    creating build/lib.linux-x86_64-3.7/shap/explainers/other\n","    copying shap/explainers/other/_maple.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","    copying shap/explainers/other/_treegain.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","    copying shap/explainers/other/__init__.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","    copying shap/explainers/other/_random.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","    copying shap/explainers/other/_lime.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","    copying shap/explainers/other/_coefficent.py -> build/lib.linux-x86_64-3.7/shap/explainers/other\n","    creating build/lib.linux-x86_64-3.7/shap/explainers/_deep\n","    copying shap/explainers/_deep/deep_tf.py -> build/lib.linux-x86_64-3.7/shap/explainers/_deep\n","    copying shap/explainers/_deep/__init__.py -> build/lib.linux-x86_64-3.7/shap/explainers/_deep\n","    copying shap/explainers/_deep/deep_pytorch.py -> build/lib.linux-x86_64-3.7/shap/explainers/_deep\n","    creating build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_partial_dependence.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_bar.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_labels.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_utils.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_decision.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_monitoring.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_text.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_image.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_beeswarm.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_force_matplotlib.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_heatmap.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/__init__.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_scatter.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_force.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_violin.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_group_difference.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_embedding.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    copying shap/plots/_waterfall.py -> build/lib.linux-x86_64-3.7/shap/plots\n","    creating build/lib.linux-x86_64-3.7/shap/plots/colors\n","    copying shap/plots/colors/__init__.py -> build/lib.linux-x86_64-3.7/shap/plots/colors\n","    copying shap/plots/colors/_colorconv.py -> build/lib.linux-x86_64-3.7/shap/plots/colors\n","    copying shap/plots/colors/_colors.py -> build/lib.linux-x86_64-3.7/shap/plots/colors\n","    creating build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/experiments.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/methods.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/__init__.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/perturbation.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/framework.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/metrics.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/plots.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/measures.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    copying shap/benchmark/models.py -> build/lib.linux-x86_64-3.7/shap/benchmark\n","    creating build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/_tabular.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/_masker.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/_text.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/_image.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/__init__.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/_fixed.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/_fixed_composite.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/_output_composite.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    copying shap/maskers/_composite.py -> build/lib.linux-x86_64-3.7/shap/maskers\n","    creating build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/_keras.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/_legacy.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/_general.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/__init__.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/_masked_model.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/transformers.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/_clustering.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/image.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    copying shap/utils/_show_progress.py -> build/lib.linux-x86_64-3.7/shap/utils\n","    creating build/lib.linux-x86_64-3.7/shap/actions\n","    copying shap/actions/_optimizer.py -> build/lib.linux-x86_64-3.7/shap/actions\n","    copying shap/actions/_action.py -> build/lib.linux-x86_64-3.7/shap/actions\n","    copying shap/actions/__init__.py -> build/lib.linux-x86_64-3.7/shap/actions\n","    creating build/lib.linux-x86_64-3.7/shap/models\n","    copying shap/models/_model.py -> build/lib.linux-x86_64-3.7/shap/models\n","    copying shap/models/_text_generation.py -> build/lib.linux-x86_64-3.7/shap/models\n","    copying shap/models/__init__.py -> build/lib.linux-x86_64-3.7/shap/models\n","    copying shap/models/_topk_lm.py -> build/lib.linux-x86_64-3.7/shap/models\n","    copying shap/models/_teacher_forcing.py -> build/lib.linux-x86_64-3.7/shap/models\n","    copying shap/models/_transformers_pipeline.py -> build/lib.linux-x86_64-3.7/shap/models\n","    creating build/lib.linux-x86_64-3.7/shap/plots/resources\n","    copying shap/plots/resources/bundle.js -> build/lib.linux-x86_64-3.7/shap/plots/resources\n","    copying shap/plots/resources/logoSmallGray.png -> build/lib.linux-x86_64-3.7/shap/plots/resources\n","    creating build/lib.linux-x86_64-3.7/shap/cext\n","    copying shap/cext/tree_shap.h -> build/lib.linux-x86_64-3.7/shap/cext\n","    running build_ext\n","    numpy.get_include() /opt/conda/lib/python3.7/site-packages/numpy/core/include\n","    building 'shap._cext' extension\n","    creating build/temp.linux-x86_64-3.7\n","    creating build/temp.linux-x86_64-3.7/shap\n","    creating build/temp.linux-x86_64-3.7/shap/cext\n","    gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.7m -I/opt/conda/lib/python3.7/site-packages/numpy/core/include -c shap/cext/_cext.cc -o build/temp.linux-x86_64-3.7/shap/cext/_cext.o\n","    gcc: error trying to exec 'cc1plus': execvp: No such file or directory\n","    error: command 'gcc' failed with exit status 1\n","    ----------------------------------------\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Command errored out with exit status 1: /opt/conda/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-e5dwqmo4/shap/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-e5dwqmo4/shap/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-k4dyove2/install-record.txt --single-version-externally-managed --compile --install-headers /opt/conda/include/python3.7m/shap Check the logs for full command output.\u001b[0m\n"]}],"source":["!pip install pycaret[full] ngboost shap"]},{"cell_type":"markdown","metadata":{"id":"yQO9VJswFGRt"},"source":["# LGBM with pycaret\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gq_6e-6RFGRv"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import random\n","import pickle\n","from pycaret.classification import *\n","from pycaret.utils import check_metric\n","import time\n","from datetime import timedelta, timezone, datetime\n","from copy import deepcopy\n","from collections import defaultdict\n","from tqdm import tqdm_notebook, tqdm\n","import json\n","from sklearn.model_selection import train_test_split, KFold, StratifiedKFold"]},{"cell_type":"markdown","metadata":{"id":"62_TwoqRFGRw"},"source":["# 데이터 가져오기"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"drlrOZ3hroWh"},"outputs":[],"source":["upper_dir = '/opt/p4-dkt-freshtomato/input/data/train_dataset'\n","train_path = f'{upper_dir}/train_data.csv'\n","# train_path = f'{upper_dir}/new_train_data.csv'\n","test_path = f'{upper_dir}/test_data.csv'\n","submission_path = f'{upper_dir}/sample_submission.csv'"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hERUJHdAAz2J"},"outputs":[],"source":["train = pd.read_csv(train_path) \n","test = pd.read_csv(test_path)\n","\n","answerCode2bool = {'userID':object,  'answerCode': 'int16', 'KnowledgeTag':object}\n","train = train.astype(answerCode2bool)\n","test = test.astype(answerCode2bool)\n","# train.Timestamp = pd.to_datetime(train.Timestamp)\n","# test.Timestamp = pd.to_datetime(test.Timestamp)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2266586, 6)\n"]}],"source":["# all_data = True\n","# if all_data == True:\n","#     train = pd.concat([train,test],axis=0)\n","#     train = train.sort_values(['userID'])\n","#     train.index = range(train.shape[0])\n","print(train.shape)"]},{"cell_type":"markdown","metadata":{"id":"sDSFqsJJFGRx"},"source":["# Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"nwJ2Vgz6EdxJ"},"source":["## Train에서 미리 추출해야할 변수들"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":951,"status":"ok","timestamp":1622563551239,"user":{"displayName":"문재훈","photoUrl":"","userId":"02977204597809886794"},"user_tz":-540},"id":"TvhQlwqnErzf","outputId":"de1f5ff9-6c6a-493d-c311-1b9f5c92ed25"},"outputs":[],"source":["# # trian에서 각 문제별 통계량 추출\n","# # 아래의 피쳐들은 train에서 뽑은 값들을 test에 대입해주어야함.\n","\n","\n","total_num_prob_in_test = train[['assessmentItemID', 'testId']].drop_duplicates().groupby('testId').size()  \n","testid_maxlen_dict = total_num_prob_in_test.to_dict()\n","\n","# LB Prediction해야하는 문항 ID들을 파악\n","set_assessmentItemID = set(test.loc[test.answerCode == -1, 'assessmentItemID'].values)\n","set_testID = set(test.loc[test.answerCode == -1, 'testId'].values)"]},{"cell_type":"markdown","metadata":{"id":"3vwnt67Tg-lt"},"source":["## Input Data에 대해서 만들어줘야 하는 피쳐들"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["asset_dir = '/opt/p4-dkt-freshtomato/input/data/train_dataset/dict_for_feature'\n","# LB prediction에서 맞춰야하는 문항 ID 파악\n","def pd_join(data, column_name, value):\n","    value = pd.DataFrame(value, columns=[f\"{column_name}\"])\n","    data = data.join(value)\n","    return data\n","def save_pickle(path, values):\n","    os.makedirs(asset_dir, exist_ok=True)\n","    with open(f\"{asset_dir}/{path}\", 'wb') as f:\n","        pickle.dump(values, f)\n","\n","def load_pickle(path):\n","    with open(f\"{asset_dir}/{path}\", 'rb') as f:\n","        load_values = pickle.load(f)\n","    return load_values\n"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Engineering"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def feature_engineering(df, is_train):\n","    if is_train:\n","        # 본 피쳐들은 train에서 얻어진 값이며 이를 저장해둡니다.\n","        testId_mean_sum_count = df.groupby(['testId'])['answerCode'].agg(['mean','sum','count']).to_dict()\n","        save_pickle(\"testId_mean_sum_count.pk\",testId_mean_sum_count)\n","\n","        assessmentItemID_mean_sum_count = df.groupby(['assessmentItemID'])['answerCode'].agg(['mean', 'sum','count']).to_dict()\n","        save_pickle(\"assessmentItemID_mean_sum_count.pk\",assessmentItemID_mean_sum_count)\n","\n","        KnowledgeTag_mean_sum_count = df.groupby(['KnowledgeTag'])['answerCode'].agg(['mean', 'sum','count']).to_dict()\n","        save_pickle(\"KnowledgeTag_mean_sum_count.pk\",KnowledgeTag_mean_sum_count)\n","\n","\n","    # 본 피쳐들은 train에서 얻어진 값을 그대로 test에도 사용합니다.\n","    # -> inference할 때는, 저장된 정답률을 가져와서 mapping하는 코드입니다.\n","    testId_mean_sum_count = load_pickle(\"testId_mean_sum_count.pk\")\n","    df[\"testId_mean\"] = df.testId.map(testId_mean_sum_count['mean'])\n","    df['testId_sum'] = df.testId.map(testId_mean_sum_count['sum'])\n","\n","    assessmentItemID_mean_sum_count = load_pickle(\"assessmentItemID_mean_sum_count.pk\")\n","    df[\"assessmentItemID_mean\"] = df.assessmentItemID.map(assessmentItemID_mean_sum_count['mean'])\n","    df['assessmentItemID_sum'] = df.assessmentItemID.map(assessmentItemID_mean_sum_count['sum'])\n","\n","    KnowledgeTag_mean_sum_count = load_pickle(\"KnowledgeTag_mean_sum_count.pk\")\n","    df[\"tag_mean\"] = df.KnowledgeTag.map(KnowledgeTag_mean_sum_count['mean'])\n","    df['tag_sum'] = df.KnowledgeTag.map(KnowledgeTag_mean_sum_count['sum'])\n","\n","    ## 문제번호 (시험지 내에서) (범주형? 연속형?) - 3자리\n","    # 0 부터 시작하도록 \n","    df['problem_number'] = df.assessmentItemID.map(lambda x: int(x[-3:])-1)\n","\n","    ## 시험지의(testID 별) 문제 최대개수가 몇인지 dict\n","    #  (이를 파악하여 중복 풀이하는 친구들을 걸러낼수있음)\n","    total_num_prob_in_test = df[['assessmentItemID', 'testId']].drop_duplicates().groupby('testId').size()\n","    testid_maxlen_dict = total_num_prob_in_test.to_dict()\n","\n","    # 시험지별 문제번호 최대값\n","    max_prob_in_test = df.groupby('testId').problem_number.max()\n","    # 시험 문제번호의 최대값과 시험지 내의 문제개수가 일치하지 않는 시험지의 testId들을 추출\n","    inconsistent_index = max_prob_in_test[max_prob_in_test +1 != total_num_prob_in_test].index  # max_prob_in_test는 0부터 시작하고, num_prob_in_test는 1부터 시작\n","    # 문제 max번호와 문제개수가 일치하지 않는 시험지에 해당하는 데이터프레임으로부터 (시험지 Id와 문제 Id) 추출 후 문제 Id의 오름차순 순서대로 정렬\n","    inconsistent_df = df.loc[df.testId.isin(inconsistent_index),['assessmentItemID', 'testId']].drop_duplicates().sort_values('assessmentItemID')      \n","\n","    # 순서대로 안 푼 시험지에서의 본래 문제의 순서가 dict에 저장된다    (inconsistent_Itemid_item_dict)\n","    inconsistent_Itemid_item_dict = {}\n","    inconsistent_df_group = inconsistent_df.groupby('testId')\n","    # 순서대로 안 푼 시험지들에 대하여 for문\n","    for key in inconsistent_df_group.groups:\n","        for i, (k,_) in enumerate(inconsistent_df_group.get_group(key).values):\n","            inconsistent_Itemid_item_dict[k] = i\n","    \n","    # origin_problem_order : 한 시험지 내에서 해당 문제 Id의 본래 순서 (중간에 비어있는 문제는 없는 문제로 생각하여 다시 순서를 매김)\n","    # ex) A080096003,A080096001,A080096005,A080096006,A080096007,A080096008,A080096002 \n","    # 문제번호 : 3, 1, 5, 6, 7, 8, 2 => 문제번호 중 4번이 빠져있음\n","    # 따라서 4번을 제외하고 문제번호를 본래순서에 대응시키면 => (1:0,2:1,3:2,5:3,6:4,7:5,8:6)\n","    # 변수생성 => origin_problem_order : (2,0,3,4,5,6,1)\n","    # 시험 문제번호의 최대값과 시험지 내의 문제개수가 일치하지 않는 시험지의 testId들을 추출        \n","    df['origin_problem_order'] =  df.assessmentItemID.map(lambda x: int(inconsistent_Itemid_item_dict[x]) if x in inconsistent_Itemid_item_dict else int(x[-3:]) - 1) # 0부터 시작하도록 -1 해줌.\n","\n","    #유저별 시퀀스를 고려하기 위해 아래와 같이 정렬\n","    df.sort_values(by=['userID','Timestamp'], inplace=True)\n","    \n","    # 유저가 푼 시험지들에 대해, 유저의 누적 정답/풀이횟수/정답률 계산\n","    df_group = df.groupby(['userID','testId'])['answerCode']\n","    df['user_total_correct_cnt'] = df_group.transform(lambda x: x.cumsum().shift(1))    # 정답개수\n","    df['user_total_ans_cnt'] = df_group.cumcount()  # 풀이횟수\n","    df['user_total_acc'] = df['user_total_correct_cnt'] / df['user_total_ans_cnt']  # 정답률\n","\n","    df['total_num_prob_in_test'] = df.testId.map(testid_maxlen_dict)    # 문제수\n","    # 특정 시험지를 몇번째 반복하여 풀고있는 것인지 계산\n","    df['nth_test'] = df['user_total_ans_cnt'] // df['total_num_prob_in_test'] # (같은)시험 완료 횟수\n","    # 유저가 풀고 있는 시험지에 대해, 현재 몇번째 문제인지\n","    df['user_test_ans_cnt'] = df['user_total_ans_cnt'] % df['total_num_prob_in_test']   # 현재 몇번째 문제인지\n","\n","    # 각 시험지 당 유저의 정확도를 계산\n","    df['user_test_correct_cnt_per_test'] = df.groupby(['userID','testId','nth_test'])['answerCode'].transform(lambda x: x.cumsum().shift(1))\n","    df['user_acc_per_test'] = df['user_test_correct_cnt_per_test']/df['user_test_ans_cnt']\n","\n","    df['int_KnowledgeTag'] = df.KnowledgeTag.astype('int64')\n","    df['test_total_id'] = df['assessmentItemID'].map(lambda x: int(x[1:7]))\n","    return df"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["test feature engineering Finished...\n"]}],"source":["\n","# df = feature_engineering(train, is_train=True)\n","# print('train feature engineering Finished...')\n","df_test = feature_engineering(test, is_train=False)\n","print('test feature engineering Finished...')\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02b88e5bcb53444782d26ae47c831c4a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userID</th>\n","      <th>assessmentItemID</th>\n","      <th>testId</th>\n","      <th>answerCode</th>\n","      <th>Timestamp</th>\n","      <th>KnowledgeTag</th>\n","      <th>testId_mean</th>\n","      <th>testId_sum</th>\n","      <th>assessmentItemID_mean</th>\n","      <th>assessmentItemID_sum</th>\n","      <th>tag_mean</th>\n","      <th>tag_sum</th>\n","      <th>problem_number</th>\n","      <th>origin_problem_order</th>\n","      <th>user_total_correct_cnt</th>\n","      <th>user_total_ans_cnt</th>\n","      <th>user_total_acc</th>\n","      <th>total_num_prob_in_test</th>\n","      <th>nth_test</th>\n","      <th>user_test_ans_cnt</th>\n","      <th>user_test_correct_cnt_per_test</th>\n","      <th>user_acc_per_test</th>\n","      <th>int_KnowledgeTag</th>\n","      <th>test_total_id</th>\n","      <th>answerCode_shift1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>A050023001</td>\n","      <td>A050000023</td>\n","      <td>1</td>\n","      <td>2020-01-09 10:56:31</td>\n","      <td>2626</td>\n","      <td>0.560944</td>\n","      <td>856</td>\n","      <td>0.646789</td>\n","      <td>141</td>\n","      <td>0.641379</td>\n","      <td>1023</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>2626</td>\n","      <td>50023</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>A050023002</td>\n","      <td>A050000023</td>\n","      <td>1</td>\n","      <td>2020-01-09 10:56:57</td>\n","      <td>2626</td>\n","      <td>0.560944</td>\n","      <td>856</td>\n","      <td>0.628440</td>\n","      <td>137</td>\n","      <td>0.641379</td>\n","      <td>1023</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>2626</td>\n","      <td>50023</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>A050023003</td>\n","      <td>A050000023</td>\n","      <td>0</td>\n","      <td>2020-01-09 10:58:31</td>\n","      <td>2625</td>\n","      <td>0.560944</td>\n","      <td>856</td>\n","      <td>0.577982</td>\n","      <td>126</td>\n","      <td>0.670013</td>\n","      <td>1535</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>1.000000</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>1.000000</td>\n","      <td>2625</td>\n","      <td>50023</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>A050023004</td>\n","      <td>A050000023</td>\n","      <td>0</td>\n","      <td>2020-01-09 10:58:36</td>\n","      <td>2625</td>\n","      <td>0.560944</td>\n","      <td>856</td>\n","      <td>0.655963</td>\n","      <td>143</td>\n","      <td>0.670013</td>\n","      <td>1535</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>0.666667</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2.0</td>\n","      <td>0.666667</td>\n","      <td>2625</td>\n","      <td>50023</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>A050023006</td>\n","      <td>A050000023</td>\n","      <td>0</td>\n","      <td>2020-01-09 10:58:43</td>\n","      <td>2623</td>\n","      <td>0.560944</td>\n","      <td>856</td>\n","      <td>0.307339</td>\n","      <td>67</td>\n","      <td>0.568970</td>\n","      <td>2314</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2.0</td>\n","      <td>4</td>\n","      <td>0.500000</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2.0</td>\n","      <td>0.500000</td>\n","      <td>2623</td>\n","      <td>50023</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>260109</th>\n","      <td>7439</td>\n","      <td>A040130001</td>\n","      <td>A040000130</td>\n","      <td>0</td>\n","      <td>2020-10-14 23:07:23</td>\n","      <td>8832</td>\n","      <td>0.644961</td>\n","      <td>832</td>\n","      <td>0.445736</td>\n","      <td>115</td>\n","      <td>0.645148</td>\n","      <td>1529</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>8832</td>\n","      <td>40130</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>260110</th>\n","      <td>7439</td>\n","      <td>A040130002</td>\n","      <td>A040000130</td>\n","      <td>1</td>\n","      <td>2020-10-14 23:07:41</td>\n","      <td>8832</td>\n","      <td>0.644961</td>\n","      <td>832</td>\n","      <td>0.476744</td>\n","      <td>123</td>\n","      <td>0.645148</td>\n","      <td>1529</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>8832</td>\n","      <td>40130</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>260111</th>\n","      <td>7439</td>\n","      <td>A040130003</td>\n","      <td>A040000130</td>\n","      <td>1</td>\n","      <td>2020-10-14 23:08:02</td>\n","      <td>8244</td>\n","      <td>0.644961</td>\n","      <td>832</td>\n","      <td>0.860465</td>\n","      <td>222</td>\n","      <td>0.767908</td>\n","      <td>804</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","      <td>0.500000</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>0.500000</td>\n","      <td>8244</td>\n","      <td>40130</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>260112</th>\n","      <td>7439</td>\n","      <td>A040130004</td>\n","      <td>A040000130</td>\n","      <td>1</td>\n","      <td>2020-10-14 23:09:31</td>\n","      <td>8244</td>\n","      <td>0.644961</td>\n","      <td>832</td>\n","      <td>0.825581</td>\n","      <td>213</td>\n","      <td>0.767908</td>\n","      <td>804</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2.0</td>\n","      <td>3</td>\n","      <td>0.666667</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2.0</td>\n","      <td>0.666667</td>\n","      <td>8244</td>\n","      <td>40130</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>260113</th>\n","      <td>7439</td>\n","      <td>A040130005</td>\n","      <td>A040000130</td>\n","      <td>-1</td>\n","      <td>2020-10-14 23:10:03</td>\n","      <td>8832</td>\n","      <td>0.644961</td>\n","      <td>832</td>\n","      <td>0.616279</td>\n","      <td>159</td>\n","      <td>0.645148</td>\n","      <td>1529</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3.0</td>\n","      <td>4</td>\n","      <td>0.750000</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>3.0</td>\n","      <td>0.750000</td>\n","      <td>8832</td>\n","      <td>40130</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>260114 rows × 25 columns</p>\n","</div>"],"text/plain":["       userID assessmentItemID      testId  answerCode            Timestamp  \\\n","0           3       A050023001  A050000023           1  2020-01-09 10:56:31   \n","1           3       A050023002  A050000023           1  2020-01-09 10:56:57   \n","2           3       A050023003  A050000023           0  2020-01-09 10:58:31   \n","3           3       A050023004  A050000023           0  2020-01-09 10:58:36   \n","4           3       A050023006  A050000023           0  2020-01-09 10:58:43   \n","...       ...              ...         ...         ...                  ...   \n","260109   7439       A040130001  A040000130           0  2020-10-14 23:07:23   \n","260110   7439       A040130002  A040000130           1  2020-10-14 23:07:41   \n","260111   7439       A040130003  A040000130           1  2020-10-14 23:08:02   \n","260112   7439       A040130004  A040000130           1  2020-10-14 23:09:31   \n","260113   7439       A040130005  A040000130          -1  2020-10-14 23:10:03   \n","\n","       KnowledgeTag  testId_mean  testId_sum  assessmentItemID_mean  \\\n","0              2626     0.560944         856               0.646789   \n","1              2626     0.560944         856               0.628440   \n","2              2625     0.560944         856               0.577982   \n","3              2625     0.560944         856               0.655963   \n","4              2623     0.560944         856               0.307339   \n","...             ...          ...         ...                    ...   \n","260109         8832     0.644961         832               0.445736   \n","260110         8832     0.644961         832               0.476744   \n","260111         8244     0.644961         832               0.860465   \n","260112         8244     0.644961         832               0.825581   \n","260113         8832     0.644961         832               0.616279   \n","\n","        assessmentItemID_sum  tag_mean  tag_sum  problem_number  \\\n","0                        141  0.641379     1023               0   \n","1                        137  0.641379     1023               1   \n","2                        126  0.670013     1535               2   \n","3                        143  0.670013     1535               3   \n","4                         67  0.568970     2314               5   \n","...                      ...       ...      ...             ...   \n","260109                   115  0.645148     1529               0   \n","260110                   123  0.645148     1529               1   \n","260111                   222  0.767908      804               2   \n","260112                   213  0.767908      804               3   \n","260113                   159  0.645148     1529               4   \n","\n","        origin_problem_order  user_total_correct_cnt  user_total_ans_cnt  \\\n","0                          0                     NaN                   0   \n","1                          1                     1.0                   1   \n","2                          2                     2.0                   2   \n","3                          3                     2.0                   3   \n","4                          5                     2.0                   4   \n","...                      ...                     ...                 ...   \n","260109                     0                     NaN                   0   \n","260110                     1                     0.0                   1   \n","260111                     2                     1.0                   2   \n","260112                     3                     2.0                   3   \n","260113                     4                     3.0                   4   \n","\n","        user_total_acc  total_num_prob_in_test  nth_test  user_test_ans_cnt  \\\n","0                  NaN                       7         0                  0   \n","1             1.000000                       7         0                  1   \n","2             1.000000                       7         0                  2   \n","3             0.666667                       7         0                  3   \n","4             0.500000                       7         0                  4   \n","...                ...                     ...       ...                ...   \n","260109             NaN                       5         0                  0   \n","260110        0.000000                       5         0                  1   \n","260111        0.500000                       5         0                  2   \n","260112        0.666667                       5         0                  3   \n","260113        0.750000                       5         0                  4   \n","\n","        user_test_correct_cnt_per_test  user_acc_per_test  int_KnowledgeTag  \\\n","0                                  NaN                NaN              2626   \n","1                                  1.0           1.000000              2626   \n","2                                  2.0           1.000000              2625   \n","3                                  2.0           0.666667              2625   \n","4                                  2.0           0.500000              2623   \n","...                                ...                ...               ...   \n","260109                             NaN                NaN              8832   \n","260110                             0.0           0.000000              8832   \n","260111                             1.0           0.500000              8244   \n","260112                             2.0           0.666667              8244   \n","260113                             3.0           0.750000              8832   \n","\n","        test_total_id  answerCode_shift1  \n","0               50023                NaN  \n","1               50023                1.0  \n","2               50023                1.0  \n","3               50023                0.0  \n","4               50023                0.0  \n","...               ...                ...  \n","260109          40130                1.0  \n","260110          40130                0.0  \n","260111          40130                1.0  \n","260112          40130                1.0  \n","260113          40130                1.0  \n","\n","[260114 rows x 25 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Add Feature considering WindowSize\n","consider_window_features = ['answerCode']\n","\n","def add_features_with_window(df,FEATS,window_size_list):\n","    empty_df = pd.DataFrame(index=range(df.shape[0]))\n","    for window_size in tqdm_notebook(window_size_list):\n","        add_df = df.groupby(['userID'])[FEATS].shift(window_size).copy().add_suffix(f'_shift{window_size}')\n","        empty_df = pd.concat([empty_df,add_df],axis=1)\n","\n","    return pd.concat([df,empty_df],axis=1)\n","\n","window_size_list=list(range(1,2))  # window size : 1 ~ \n","# df = add_features_with_window(df,consider_window_features,window_size_list)\n","df_test = add_features_with_window(df_test,consider_window_features,window_size_list)\n","df_test"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"jjt90EavVzji"},"outputs":[{"name":"stdout","output_type":"stream","text":["df_csv_name_after_FE : /opt/p4-dkt-freshtomato/input/data/train_dataset/only_train_25개_원래변수들.csv\n"]}],"source":["df_csv_name_after_FE = f'{upper_dir}/only_train_25개_원래변수들.csv'\n","# df.to_csv(df_csv_name_after_FE,index=False)\n","df = pd.read_csv(df_csv_name_after_FE)\n","print(f\"df_csv_name_after_FE : {df_csv_name_after_FE}\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":399,"status":"ok","timestamp":1622563911840,"user":{"displayName":"문재훈","photoUrl":"","userId":"02977204597809886794"},"user_tz":-540},"id":"qP3LS8_PU1vH","outputId":"7de71b0b-758a-4a51-a3be-dd59f5a118e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2266586, 25)\n"]}],"source":["print(df.shape)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"V1Ct8LUX-shj"},"outputs":[],"source":["# LB 제출해야하는 testset\n","df_test_shift = df_test[df_test['userID'] != df_test['userID'].shift(-1)]"]},{"cell_type":"markdown","metadata":{"id":"wFLTDadShGYT"},"source":["## 엔지니어링 이후의 EDA"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137,"referenced_widgets":["f721ab5cdd524830861948c508b02cf5","3cd74ccffbf242c0afa2232bfa85ed06","f14b908c6a864e8ea051b15e61095076","c29715ad8e3349a79c20678dd0468562","f494c699f88d4576bc6b988bb2044920","ed101556176343888bcd898dc17ff9e6","ccc9698663bf43868b76712e454229fb","1f595e8f870a4edfb9642dd1456d1c15"]},"executionInfo":{"elapsed":66142,"status":"ok","timestamp":1622573644569,"user":{"displayName":"문재훈","photoUrl":"","userId":"02977204597809886794"},"user_tz":-540},"id":"v5zP7xp3Y1JZ","outputId":"8fa789a4-af82-4fba-ea2a-fa8e5689ff84"},"outputs":[{"name":"stdout","output_type":"stream","text":["User 별 마지막 row에 결측치가 존재하는 Column\n","{}\n","User 별 마지막 row에 결측치가 존재하는 Column\n","{}\n"]}],"source":["check_df = df[df['userID'] != df['userID'].shift(-1)]\n","nan_check_per_column_dict = check_df.isnull().sum().to_dict()\n","print('User 별 마지막 row에 결측치가 존재하는 Column')\n","over_zero_nan_per_column = {k:v for k,v in nan_check_per_column_dict.items() if v!=0}\n","print(over_zero_nan_per_column)\n","\n","check_df = df_test[df_test['userID'] != df_test['userID'].shift(-1)]\n","nan_check_per_column_dict = check_df.isnull().sum().to_dict()\n","print('User 별 마지막 row에 결측치가 존재하는 Column')\n","over_zero_nan_per_column = {k:v for k,v in nan_check_per_column_dict.items() if v!=0}\n","print(over_zero_nan_per_column)\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["User 별 첫 row에 결측치가 존재하는 Column\n","{'user_total_correct_cnt': 6698, 'user_total_acc': 6698, 'user_test_correct_cnt_per_test': 6698, 'user_acc_per_test': 6698, 'answerCode_shift1': 6698}\n","User 별 첫 row에 결측치가 존재하는 Column\n","{'user_total_correct_cnt': 744, 'user_total_acc': 744, 'user_test_correct_cnt_per_test': 744, 'user_acc_per_test': 744, 'answerCode_shift1': 744}\n"]}],"source":["check_df = df[df['userID'] != df['userID'].shift(1)]\n","nan_check_per_column_dict = check_df.isnull().sum().to_dict()\n","print('User 별 첫 row에 결측치가 존재하는 Column')\n","over_zero_nan_per_column = {k:v for k,v in nan_check_per_column_dict.items() if v!=0}\n","print(over_zero_nan_per_column)\n","\n","check_df = df_test[df_test['userID'] != df_test['userID'].shift(1)]\n","nan_check_per_column_dict = check_df.isnull().sum().to_dict()\n","print('User 별 첫 row에 결측치가 존재하는 Column')\n","over_zero_nan_per_column = {k:v for k,v in nan_check_per_column_dict.items() if v!=0}\n","print(over_zero_nan_per_column)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":327,"status":"ok","timestamp":1622574217308,"user":{"displayName":"문재훈","photoUrl":"","userId":"02977204597809886794"},"user_tz":-540},"id":"9_rR_FGG9T5u","outputId":"2599f13f-2a18-49bf-8d40-33cb846e41fb"},"outputs":[{"data":{"text/plain":["{'user_total_correct_cnt': 365164,\n"," 'user_total_acc': 365164,\n"," 'user_test_correct_cnt_per_test': 372603,\n"," 'user_acc_per_test': 372603,\n"," 'answerCode_shift1': 6698}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# 모든 데이터셋 row에 대하여 결측치가 1개 이상인 column, 결측치 개수\n","{k:v for k,v in df.isnull().sum().to_dict().items() if v!=0}"]},{"cell_type":"markdown","metadata":{"id":"FloWAa2IFGRy"},"source":["# Split Train valid Set"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["is_all_data = False\n","pre_split_beforehand = False # '', 'stratify_split' train과 holdout 나눌때 stratify 기준으로 먼저 나누고 시작하기\n","valid_only_LB = 'problem'   # '', 'problem', 'test', # prediction에 사용할 test row들의 (문제ID, 시험지ID)에 해당하는 row만 추출하여 Valid한다.\n","train_except_holdout = 'False'   # 'index', 'user' # holdout과 train을 겹치지 않게 할지, 그렇다면 그 기준은 무엇으로 할지\n","train_only_LB = 'problem'   # '', 'problem', 'test' # prediction에 사용할 test row들의 (문제ID, 시험지ID)에 해당하는 row만 추출하여 Train한다.\n","# train_with_finalrow = None\n","train_with_finalrow = 'False'  # None 'test','user' # 시험지 혹은 유저별 마지막 row만 가져와서 학습할 것인지\n","seed = 42"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"1cvgUhyvMO12"},"outputs":[],"source":["def train_holdout_split(\n","                        df,\n","                        is_all_data=is_all_data,  # input df가 train과 test 모두인지,\n","                        pre_split_beforehand=pre_split_beforehand,\n","                        valid_only_LB=valid_only_LB,\n","                        train_except_holdout=train_except_holdout,\n","                        train_with_finalrow = train_with_finalrow,\n","                        train_only_LB = train_only_LB,\n","                        ):\n","\n","    # train과 test를 합쳐 사용한다면 answerCode==-1에 해당하는 row는 모두 지워준다.\n","    if is_all_data:\n","        copy_df = df[df.answerCode != -1].copy()\n","    else:\n","        copy_df = df.copy()\n","    print(copy_df.shape)\n","    if pre_split_beforehand == 'stratify_split':\n","        split_idx = list(dict.fromkeys(copy_df.userID))\n","        ## stratified split을 하기 위한 기준을 설정한다.\n","        stratify_key = copy_df.groupby(['userID'])['answerCode'].apply(lambda x: x.iloc[-1])\n","        tr_val_idx, holdout_idx = train_test_split(split_idx, test_size=0.2,random_state=seed,shuffle=True,stratify=stratify_key)\n","    else:\n","        tr_val_idx = (copy_df['userID'] == copy_df['userID'])\n","        holdout_idx = (copy_df['userID'] == copy_df['userID'])\n","    print(all(tr_val_idx))\n","    print(len(tr_val_idx))\n","    print(len(holdout_idx))\n","\n","    # 유저별 마지막 row에 해당하는 데이터만 Holdout으로 사용\n","    # 추가적으로 LB Test set과 동일 조건의 row만 Valid에서 추출할지 결정\n","    # 빈 값이 아니라면, 추가조건으로 리더보드와 동일 문제ID에 해당하는 row만 추출하여 holdout로 사용\n","    if valid_only_LB=='problem':\n","        holdout_filtered_idxes = (copy_df['userID'] != copy_df['userID'].shift(-1)) & (copy_df.assessmentItemID.isin(set_assessmentItemID)) & holdout_idx\n","    elif valid_only_LB=='test':\n","        holdout_filtered_idxes = (copy_df['userID'] != copy_df['userID'].shift(-1)) & (copy_df.testId.isin(set_testID)) & holdout_idx\n","    else:\n","        holdout_filtered_idxes = (copy_df['userID'] != copy_df['userID'].shift(-1)) & holdout_idx\n","\n","    holdout = copy_df[holdout_filtered_idxes].copy()\n","\n","    print(f\"set(holdout.userID) : {len(set(holdout.userID))}\")\n","    print(f\"set(holdout.index) : {len(set(holdout.index))}\")\n","    if train_except_holdout=='user':\n","        tr_val_filtered_idxes = (copy_df['userID'].isin(set(holdout.userID)) == False) & tr_val_idx\n","    elif train_except_holdout=='index':\n","        tr_val_filtered_idxes = (copy_df.index.isin(set(holdout.index)) == False) & tr_val_idx\n","    else:\n","        tr_val_filtered_idxes = tr_val_idx\n","    \n","    tr_val = copy_df[tr_val_filtered_idxes].copy()\n","\n","    print(f\"(tr_val.shape) : {tr_val.shape}\")\n","\n","    # 학습시 무엇을 기준으로 마지막 row만 가져올지\n","    if train_with_finalrow == 'test':\n","        tr_val = tr_val[tr_val['testId'] != tr_val['testId'].shift(-1)].copy()\n","    elif train_with_finalrow == 'user':\n","        tr_val = tr_val[tr_val['userID'] != tr_val['userID'].shift(-1)].copy()\n","    else:\n","        pass\n","\n","    # LB prediction의 row에서 맞혀야하는 문제들만 학습할 것인지 (444개)\n","    if train_only_LB=='problem':\n","        tr_val = tr_val[tr_val.assessmentItemID.isin(set_assessmentItemID)].copy()\n","    # LB prediction의 row에서 맞혀야하는 시험지들만 학습할 것인지 (411개)\n","    elif train_only_LB=='test':\n","        tr_val = tr_val[tr_val.testId.isin(set_testID)].copy()\n","    else:\n","        pass\n","\n","    return tr_val, holdout"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2266586, 25)\n","True\n","2266586\n","2266586\n","set(holdout.userID) : 4775\n","set(holdout.index) : 4775\n","(tr_val.shape) : (2266586, 25)\n","====최종적인 len(tr_val), len(holdout)====\n","107202 4775\n","0.5593832204623048 0.47706806282722514\n"]}],"source":["fold = 10\n","method = 'soft' # 'soft' , 'hard' : 모델들 blend할 때, 앙상블방법\n","\n","tr_val, holdout = train_holdout_split(\n","                                    df, \n","                                    is_all_data=is_all_data,\n","                                    pre_split_beforehand=pre_split_beforehand,\n","                                    valid_only_LB=valid_only_LB,\n","                                    train_except_holdout=train_except_holdout,\n","                                    train_with_finalrow = train_with_finalrow,\n","                                    train_only_LB = train_only_LB,\n","                                    )\n","\n","print('====최종적인 len(tr_val), len(holdout)====')\n","print(len(tr_val), len(holdout))\n","print(tr_val.answerCode.mean(), holdout.answerCode.mean())\n"]},{"cell_type":"markdown","metadata":{"id":"npqAI40BIMnK"},"source":["# AUTOML (NGBoosting도 쓸 수 있으니 참고)"]},{"cell_type":"markdown","metadata":{},"source":["## 모델, grid score df, plot, config, output csv 파일들이 저장될 상위 경로"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["upper_dir_of_pycaret : /opt/p4-dkt-freshtomato/output_pycaret/0614_0553\n"]}],"source":["# 모델들, grid score df, plot 등이 모두 저장되는 상위경로\n","now_time = datetime.now(timezone(timedelta(hours=9))).strftime('%m%d_%H%M')\n","upper_dir_of_pycaret = f'/opt/p4-dkt-freshtomato/output_pycaret/{now_time}'\n","print(f\"upper_dir_of_pycaret : {upper_dir_of_pycaret}\")"]},{"cell_type":"markdown","metadata":{},"source":["##  각 단계마다 (create - tune - ensemble - finalize) 모델, 정보 저장해주는 코드"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def plot_save_pycaret_model(name, model,mode='tune'):\n","    # Feature Importance Plot 저장하기\n","    plot_dir = f'{upper_dir_of_pycaret}/plot_result'\n","    os.makedirs(plot_dir, exist_ok=True)\n","    os.chdir(plot_dir)\n","    # top10 feature\n","    feature_importance_plot = plot_model(model, plot = 'feature', save=True)\n","    os.rename(f'{plot_dir}/Feature Importance.png',f'{plot_dir}/{mode}_{name}_10_feature_importance.png')\n","    # all feature\n","    feature_importance_plot = plot_model(model, plot = 'feature_all', save=True)\n","    os.rename(f'{plot_dir}/Feature Importance (All).png',f'{plot_dir}/{mode}_{name}_all_feature_importance.png')\n","\n","# def create_and_save_pycaret_model(name):\n","#     created_model = create_model(name, cross_validation = True)\n","#     # Display되는 score grid dataframe도 가져올 수가 있다. \n","#     df_result_model = pull()\n","#     df_result_model.to_csv(f'{upper_dir_of_pycaret}/{name}_create_grid_df.csv')\n","#     # 실험결과를 저장하는 것도 가능하다.\n","#     save_experiment(f'{upper_dir_of_pycaret}/{name}_create_exp_result')\n","#     # 모델을 저장하는 것도 가능하다.\n","#     save_model(created_model, f'{upper_dir_of_pycaret}/{name}_model_saved')\n","#     return created_model\n","\n","def tune_and_save_pycaret_model(name, model, optimize = 'AUC', fold = fold, n_iter = 10):\n","    print('Now Tuning the models....')\n","    tuned_model = tune_model(model, optimize = optimize, fold = fold,)   # n_iter = n_iter\n","    # Display되는 score grid dataframe도 가져올 수가 있다. \n","    df_result_model = pull()\n","    os.makedirs(f\"{upper_dir_of_pycaret}/grid_df\",exist_ok=True)\n","    df_result_model.to_csv(f'{upper_dir_of_pycaret}/grid_df/{name}_tuned_grid_df.csv')\n","    # 모델의 각종 plot을 저장하는 것도 가능하다.\n","    plot_save_pycaret_model(name, tuned_model, mode='tune')\n","    # 실험결과를 저장하는 것도 가능하다.\n","    os.makedirs(f\"{upper_dir_of_pycaret}/expr\",exist_ok=True)\n","    try:\n","        save_experiment(f'{upper_dir_of_pycaret}/expr/{name}_tuned_exp_result')\n","    except:\n","        print('tuned model의 실험결과는 저장할 수 없습니다.')\n","    # 모델을 저장하는 것도 가능하다.\n","    os.makedirs(f\"{upper_dir_of_pycaret}/models\",exist_ok=True)\n","    save_model(tuned_model, f'{upper_dir_of_pycaret}/models/{name}_tuned_model_saved')\n","    return tuned_model\n","\n","def ensemble_and_save_pycaret_model(\n","                                    model_name_list,\n","                                    estimator_list,\n","                                    fold = fold,\n","                                    optimize = 'AUC',\n","                                    method = method,\n","                                    meta_model = None,\n","                                    mode = None,\n","                                    ):\n","    print('Now Ensemble the models....')\n","    if mode=='blend_models':\n","        ensembled_model = blend_models(estimator_list = estimator_list, fold = fold, optimize = optimize, method = method)\n","    elif mode=='stack_models':\n","        ensembled_model = stack_models(estimator_list = estimator_list, fold = fold, optimize = optimize, meta_model = meta_model)\n","    # Display되는 score grid dataframe도 가져올 수가 있다. \n","    df_result_model = pull()\n","    name = '_'.join(model_name_list)\n","    os.makedirs(f\"{upper_dir_of_pycaret}/grid_df\",exist_ok=True)\n","    df_result_model.to_csv(f'{upper_dir_of_pycaret}/grid_df/{name}_ensembled_grid_df.csv')\n","    # 모델의 각종 plot을 저장하는 것도 가능하다.\n","    try:\n","        plot_save_pycaret_model(name, ensembled_model, mode='tune')\n","    except:\n","        print(f'ensemble model은 변수중요도 plot을 저장할 수 없습니다')\n","    # 실험결과를 저장하는 것도 가능하다.\n","    os.makedirs(f\"{upper_dir_of_pycaret}/expr\",exist_ok=True)\n","    try:\n","        save_experiment(f'{upper_dir_of_pycaret}/expr/{name}_ensembled_exp_result')\n","    except:\n","        print(f'ensemble model은 실험결과를 저장할 수 없습니다')\n","    # 모델을 저장하는 것도 가능하다.\n","    os.makedirs(f\"{upper_dir_of_pycaret}/models\",exist_ok=True)\n","    save_model(ensembled_model, f'{upper_dir_of_pycaret}/models/{name}_ensembled_model_saved')\n","    return ensembled_model\n","\n","def finalize_and_save_pycaret_model(model_name_list,model,):\n","    print('Now Finalizing the model....')\n","    finalized_model = finalize_model(model)\n","    # Display되는 score grid dataframe도 가져올 수가 있다. \n","    df_result_model = pull()\n","    name = '_'.join(model_name_list)\n","    os.makedirs(f\"{upper_dir_of_pycaret}/grid_df\",exist_ok=True)\n","    df_result_model.to_csv(f'{upper_dir_of_pycaret}/grid_df/{name}_finalized_grid_df.csv')\n","    # 모델의 각종 plot을 저장하는 것도 가능하다.\n","    try:\n","        plot_save_pycaret_model(name, finalized_model, mode='tune')\n","    except:\n","        print(f'final model은 변수중요도 plot을 저장할 수 없습니다')\n","    # 실험결과를 저장하는 것도 가능하다.\n","    os.makedirs(f\"{upper_dir_of_pycaret}/expr\",exist_ok=True)\n","    try:\n","        save_experiment(f'{upper_dir_of_pycaret}/expr/{name}_finalized_exp_result')\n","    except:\n","        print(f'final model은 실험결과를 저장할 수 없습니다')\n","    # 모델을 저장하는 것도 가능하다.\n","    os.makedirs(f\"{upper_dir_of_pycaret}/models\",exist_ok=True)\n","    save_model(finalized_model, f'{upper_dir_of_pycaret}/models/{name}_finalized_model_saved')\n","    return finalized_model\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["stratified=True\r\n","class LastSolveKFold:\r\n","    def __init__(self,\r\n","                n_splits=fold,\r\n","                shuffle=True,\r\n","                random_state=42\r\n","                ):\r\n","        self.n_splits = n_splits\r\n","        self.shuffle = shuffle\r\n","        self.random_state = random_state\r\n","\r\n","    def split(self, X, y=None, groups=None):\r\n","        # print(X.head())\r\n","        # print(X['userID'])\r\n","        user_id = np.array(tr_val['userID'])\r\n","        X, y, groups = [np.array(iterable) for iterable in (X, y, groups)]\r\n","        n_samples = len(X)\r\n","        #########################################\r\n","        last_user_index = np.where(np.roll(user_id, shift=-1) != user_id)[0]\r\n","        stratify_key = tr_val.iloc[last_user_index].answerCode\r\n","        # stratify_key = tr_val.groupby(['userID'])['answerCode'].mean().apply(lambda x: 1 if x>0.61 else 0)\r\n","        total_val_indices = []\r\n","        total_indices = np.arange(n_samples)\r\n","        print(f\"last_user_index : {len(last_user_index)}\")\r\n","        print(f\"total_indices : {len(total_indices)}\")\r\n","        print(f\"len(total_indices)-len(last_user_index)//10 : {len(total_indices)-len(last_user_index)//10}\")\r\n","        if stratified==True:\r\n","            kfold_split = StratifiedKFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state = self.random_state).split(last_user_index,stratify_key)\r\n","        else:\r\n","            kfold_split = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state = self.random_state).split(last_user_index)\r\n","\r\n","        for _, test_idx in tqdm(kfold_split):\r\n","            test_idx = last_user_index[test_idx]\r\n","            total_val_indices.extend(test_idx)\r\n","            # train_idx = [tr_id for tr_id in total_indices if not tr_id in test_idx]\r\n","            train_idx = np.setdiff1d(total_indices, test_idx)\r\n","            print(f'train_idx : {len(train_idx)}')\r\n","            # print(f'train_idx : {type(train_idx)}')\r\n","            print(f'val_idx : {len(test_idx)}')\r\n","            # print(f'val_idx : {type(test_idx)}')\r\n","            # print(f'val_idx : {test_idx}')\r\n","            print(f'shape of X : {X.shape}')\r\n","            yield np.array(train_idx), np.array(test_idx)\r\n","        print(f\"valid Set이 된 모든 row들의 개수 : {len(total_val_indices)}\")\r\n","        print(f\"valid Set이 된 모든 row들 unique 개수 : {len(set(total_val_indices))}\")\r\n","\r\n","    def get_n_splits(self, X, y=None, groups=None):\r\n","        return self.n_splits\r\n","\r\n","last_fold = LastSolveKFold(n_splits=10, shuffle=True, random_state=42)\r\n"]},{"cell_type":"markdown","metadata":{},"source":["# Pycaret tabular.py setup 함수 내부 수정\n","train_size에 상관없이 모든 데이터를 사용하도록"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['userID', 'assessmentItemID', 'testId', 'answerCode', 'Timestamp',\n","       'KnowledgeTag', 'testId_mean', 'testId_sum', 'assessmentItemID_mean',\n","       'assessmentItemID_sum', 'tag_mean', 'tag_sum', 'problem_number',\n","       'origin_problem_order', 'user_total_correct_cnt', 'user_total_ans_cnt',\n","       'user_total_acc', 'total_num_prob_in_test', 'nth_test',\n","       'user_test_ans_cnt', 'user_test_correct_cnt_per_test',\n","       'user_acc_per_test', 'int_KnowledgeTag', 'test_total_id',\n","       'answerCode_shift1'],\n","      dtype='object')"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/html":["<style  type=\"text/css\" >\n","#T_6c06e_row5_col1{\n","            background-color:  lightgreen;\n","        }</style><table id=\"T_6c06e_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Description</th>        <th class=\"col_heading level0 col1\" >Value</th>    </tr></thead><tbody>\n","                <tr>\n","                        <th id=\"T_6c06e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","                        <td id=\"T_6c06e_row0_col0\" class=\"data row0 col0\" >session_id</td>\n","                        <td id=\"T_6c06e_row0_col1\" class=\"data row0 col1\" >7734</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","                        <td id=\"T_6c06e_row1_col0\" class=\"data row1 col0\" >Target</td>\n","                        <td id=\"T_6c06e_row1_col1\" class=\"data row1 col1\" >answerCode</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","                        <td id=\"T_6c06e_row2_col0\" class=\"data row2 col0\" >Target Type</td>\n","                        <td id=\"T_6c06e_row2_col1\" class=\"data row2 col1\" >Binary</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","                        <td id=\"T_6c06e_row3_col0\" class=\"data row3 col0\" >Label Encoded</td>\n","                        <td id=\"T_6c06e_row3_col1\" class=\"data row3 col1\" >0: 0, 1: 1</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","                        <td id=\"T_6c06e_row4_col0\" class=\"data row4 col0\" >Original Data</td>\n","                        <td id=\"T_6c06e_row4_col1\" class=\"data row4 col1\" >(107202, 21)</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n","                        <td id=\"T_6c06e_row5_col0\" class=\"data row5 col0\" >Missing Values</td>\n","                        <td id=\"T_6c06e_row5_col1\" class=\"data row5 col1\" >True</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n","                        <td id=\"T_6c06e_row6_col0\" class=\"data row6 col0\" >Numeric Features</td>\n","                        <td id=\"T_6c06e_row6_col1\" class=\"data row6 col1\" >13</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n","                        <td id=\"T_6c06e_row7_col0\" class=\"data row7 col0\" >Categorical Features</td>\n","                        <td id=\"T_6c06e_row7_col1\" class=\"data row7 col1\" >6</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n","                        <td id=\"T_6c06e_row8_col0\" class=\"data row8 col0\" >Ordinal Features</td>\n","                        <td id=\"T_6c06e_row8_col1\" class=\"data row8 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n","                        <td id=\"T_6c06e_row9_col0\" class=\"data row9 col0\" >High Cardinality Features</td>\n","                        <td id=\"T_6c06e_row9_col1\" class=\"data row9 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n","                        <td id=\"T_6c06e_row10_col0\" class=\"data row10 col0\" >High Cardinality Method</td>\n","                        <td id=\"T_6c06e_row10_col1\" class=\"data row10 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n","                        <td id=\"T_6c06e_row11_col0\" class=\"data row11 col0\" >Transformed Train Set</td>\n","                        <td id=\"T_6c06e_row11_col1\" class=\"data row11 col1\" >(107202, 114)</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n","                        <td id=\"T_6c06e_row12_col0\" class=\"data row12 col0\" >Transformed Test Set</td>\n","                        <td id=\"T_6c06e_row12_col1\" class=\"data row12 col1\" >(10721, 114)</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n","                        <td id=\"T_6c06e_row13_col0\" class=\"data row13 col0\" >Shuffle Train-Test</td>\n","                        <td id=\"T_6c06e_row13_col1\" class=\"data row13 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n","                        <td id=\"T_6c06e_row14_col0\" class=\"data row14 col0\" >Stratify Train-Test</td>\n","                        <td id=\"T_6c06e_row14_col1\" class=\"data row14 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n","                        <td id=\"T_6c06e_row15_col0\" class=\"data row15 col0\" >Fold Generator</td>\n","                        <td id=\"T_6c06e_row15_col1\" class=\"data row15 col1\" >LastSolveKFold</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n","                        <td id=\"T_6c06e_row16_col0\" class=\"data row16 col0\" >Fold Number</td>\n","                        <td id=\"T_6c06e_row16_col1\" class=\"data row16 col1\" >10</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n","                        <td id=\"T_6c06e_row17_col0\" class=\"data row17 col0\" >CPU Jobs</td>\n","                        <td id=\"T_6c06e_row17_col1\" class=\"data row17 col1\" >-1</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n","                        <td id=\"T_6c06e_row18_col0\" class=\"data row18 col0\" >Use GPU</td>\n","                        <td id=\"T_6c06e_row18_col1\" class=\"data row18 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n","                        <td id=\"T_6c06e_row19_col0\" class=\"data row19 col0\" >Log Experiment</td>\n","                        <td id=\"T_6c06e_row19_col1\" class=\"data row19 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n","                        <td id=\"T_6c06e_row20_col0\" class=\"data row20 col0\" >Experiment Name</td>\n","                        <td id=\"T_6c06e_row20_col1\" class=\"data row20 col1\" >clf-default-name</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n","                        <td id=\"T_6c06e_row21_col0\" class=\"data row21 col0\" >USI</td>\n","                        <td id=\"T_6c06e_row21_col1\" class=\"data row21 col1\" >77c2</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n","                        <td id=\"T_6c06e_row22_col0\" class=\"data row22 col0\" >Imputation Type</td>\n","                        <td id=\"T_6c06e_row22_col1\" class=\"data row22 col1\" >simple</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n","                        <td id=\"T_6c06e_row23_col0\" class=\"data row23 col0\" >Iterative Imputation Iteration</td>\n","                        <td id=\"T_6c06e_row23_col1\" class=\"data row23 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n","                        <td id=\"T_6c06e_row24_col0\" class=\"data row24 col0\" >Numeric Imputer</td>\n","                        <td id=\"T_6c06e_row24_col1\" class=\"data row24 col1\" >mean</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n","                        <td id=\"T_6c06e_row25_col0\" class=\"data row25 col0\" >Iterative Imputation Numeric Model</td>\n","                        <td id=\"T_6c06e_row25_col1\" class=\"data row25 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n","                        <td id=\"T_6c06e_row26_col0\" class=\"data row26 col0\" >Categorical Imputer</td>\n","                        <td id=\"T_6c06e_row26_col1\" class=\"data row26 col1\" >constant</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n","                        <td id=\"T_6c06e_row27_col0\" class=\"data row27 col0\" >Iterative Imputation Categorical Model</td>\n","                        <td id=\"T_6c06e_row27_col1\" class=\"data row27 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n","                        <td id=\"T_6c06e_row28_col0\" class=\"data row28 col0\" >Unknown Categoricals Handling</td>\n","                        <td id=\"T_6c06e_row28_col1\" class=\"data row28 col1\" >least_frequent</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n","                        <td id=\"T_6c06e_row29_col0\" class=\"data row29 col0\" >Normalize</td>\n","                        <td id=\"T_6c06e_row29_col1\" class=\"data row29 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n","                        <td id=\"T_6c06e_row30_col0\" class=\"data row30 col0\" >Normalize Method</td>\n","                        <td id=\"T_6c06e_row30_col1\" class=\"data row30 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n","                        <td id=\"T_6c06e_row31_col0\" class=\"data row31 col0\" >Transformation</td>\n","                        <td id=\"T_6c06e_row31_col1\" class=\"data row31 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n","                        <td id=\"T_6c06e_row32_col0\" class=\"data row32 col0\" >Transformation Method</td>\n","                        <td id=\"T_6c06e_row32_col1\" class=\"data row32 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n","                        <td id=\"T_6c06e_row33_col0\" class=\"data row33 col0\" >PCA</td>\n","                        <td id=\"T_6c06e_row33_col1\" class=\"data row33 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n","                        <td id=\"T_6c06e_row34_col0\" class=\"data row34 col0\" >PCA Method</td>\n","                        <td id=\"T_6c06e_row34_col1\" class=\"data row34 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n","                        <td id=\"T_6c06e_row35_col0\" class=\"data row35 col0\" >PCA Components</td>\n","                        <td id=\"T_6c06e_row35_col1\" class=\"data row35 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n","                        <td id=\"T_6c06e_row36_col0\" class=\"data row36 col0\" >Ignore Low Variance</td>\n","                        <td id=\"T_6c06e_row36_col1\" class=\"data row36 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n","                        <td id=\"T_6c06e_row37_col0\" class=\"data row37 col0\" >Combine Rare Levels</td>\n","                        <td id=\"T_6c06e_row37_col1\" class=\"data row37 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n","                        <td id=\"T_6c06e_row38_col0\" class=\"data row38 col0\" >Rare Level Threshold</td>\n","                        <td id=\"T_6c06e_row38_col1\" class=\"data row38 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n","                        <td id=\"T_6c06e_row39_col0\" class=\"data row39 col0\" >Numeric Binning</td>\n","                        <td id=\"T_6c06e_row39_col1\" class=\"data row39 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n","                        <td id=\"T_6c06e_row40_col0\" class=\"data row40 col0\" >Remove Outliers</td>\n","                        <td id=\"T_6c06e_row40_col1\" class=\"data row40 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n","                        <td id=\"T_6c06e_row41_col0\" class=\"data row41 col0\" >Outliers Threshold</td>\n","                        <td id=\"T_6c06e_row41_col1\" class=\"data row41 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n","                        <td id=\"T_6c06e_row42_col0\" class=\"data row42 col0\" >Remove Multicollinearity</td>\n","                        <td id=\"T_6c06e_row42_col1\" class=\"data row42 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n","                        <td id=\"T_6c06e_row43_col0\" class=\"data row43 col0\" >Multicollinearity Threshold</td>\n","                        <td id=\"T_6c06e_row43_col1\" class=\"data row43 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n","                        <td id=\"T_6c06e_row44_col0\" class=\"data row44 col0\" >Clustering</td>\n","                        <td id=\"T_6c06e_row44_col1\" class=\"data row44 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n","                        <td id=\"T_6c06e_row45_col0\" class=\"data row45 col0\" >Clustering Iteration</td>\n","                        <td id=\"T_6c06e_row45_col1\" class=\"data row45 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n","                        <td id=\"T_6c06e_row46_col0\" class=\"data row46 col0\" >Polynomial Features</td>\n","                        <td id=\"T_6c06e_row46_col1\" class=\"data row46 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n","                        <td id=\"T_6c06e_row47_col0\" class=\"data row47 col0\" >Polynomial Degree</td>\n","                        <td id=\"T_6c06e_row47_col1\" class=\"data row47 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n","                        <td id=\"T_6c06e_row48_col0\" class=\"data row48 col0\" >Trignometry Features</td>\n","                        <td id=\"T_6c06e_row48_col1\" class=\"data row48 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n","                        <td id=\"T_6c06e_row49_col0\" class=\"data row49 col0\" >Polynomial Threshold</td>\n","                        <td id=\"T_6c06e_row49_col1\" class=\"data row49 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n","                        <td id=\"T_6c06e_row50_col0\" class=\"data row50 col0\" >Group Features</td>\n","                        <td id=\"T_6c06e_row50_col1\" class=\"data row50 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n","                        <td id=\"T_6c06e_row51_col0\" class=\"data row51 col0\" >Feature Selection</td>\n","                        <td id=\"T_6c06e_row51_col1\" class=\"data row51 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n","                        <td id=\"T_6c06e_row52_col0\" class=\"data row52 col0\" >Feature Selection Method</td>\n","                        <td id=\"T_6c06e_row52_col1\" class=\"data row52 col1\" >classic</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n","                        <td id=\"T_6c06e_row53_col0\" class=\"data row53 col0\" >Features Selection Threshold</td>\n","                        <td id=\"T_6c06e_row53_col1\" class=\"data row53 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row54\" class=\"row_heading level0 row54\" >54</th>\n","                        <td id=\"T_6c06e_row54_col0\" class=\"data row54 col0\" >Feature Interaction</td>\n","                        <td id=\"T_6c06e_row54_col1\" class=\"data row54 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row55\" class=\"row_heading level0 row55\" >55</th>\n","                        <td id=\"T_6c06e_row55_col0\" class=\"data row55 col0\" >Feature Ratio</td>\n","                        <td id=\"T_6c06e_row55_col1\" class=\"data row55 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row56\" class=\"row_heading level0 row56\" >56</th>\n","                        <td id=\"T_6c06e_row56_col0\" class=\"data row56 col0\" >Interaction Threshold</td>\n","                        <td id=\"T_6c06e_row56_col1\" class=\"data row56 col1\" >None</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row57\" class=\"row_heading level0 row57\" >57</th>\n","                        <td id=\"T_6c06e_row57_col0\" class=\"data row57 col0\" >Fix Imbalance</td>\n","                        <td id=\"T_6c06e_row57_col1\" class=\"data row57 col1\" >False</td>\n","            </tr>\n","            <tr>\n","                        <th id=\"T_6c06e_level0_row58\" class=\"row_heading level0 row58\" >58</th>\n","                        <td id=\"T_6c06e_row58_col0\" class=\"data row58 col0\" >Fix Imbalance Method</td>\n","                        <td id=\"T_6c06e_row58_col1\" class=\"data row58 col1\" >SMOTE</td>\n","            </tr>\n","    </tbody></table>"],"text/plain":["<pandas.io.formats.style.Styler at 0x7fe2ab3241d0>"]},"metadata":{},"output_type":"display_data"}],"source":["experiment_name = 'LGBM-mjh-21_feature_Random-10fold'\n","FEATS = ['answerCode', 'Timestamp',\n","       'KnowledgeTag', 'problem_number', 'origin_problem_order',\n","       'user_total_correct_cnt', 'user_total_ans_cnt', 'user_total_acc',\n","       'total_num_prob_in_test', 'nth_test', 'user_test_ans_cnt',\n","       'user_test_correct_cnt_per_test', 'user_acc_per_test', 'testId_mean',\n","       'testId_sum', 'assessmentItemID_mean', 'assessmentItemID_sum', 'tag_mean', 'tag_sum',\n","       'int_KnowledgeTag', 'test_total_id']\n","\n","cat_features=[]\n","continuous_features=[]\n","\n","print(f'FEATS : {len(FEATS)}')\n","print(f\"target : 'answerCode'\")\n","print(f'cat_features : {len(cat_features)}')\n","print(f'continuous_features : {len(continuous_features)}')\n","print(f\"etc_features : {[f for f in FEATS if f not in cat_features+continuous_features+['answerCode']]}\")\n","\n","## Classification Models :  https://github.com/pycaret/pycaret/blob/master/pycaret/containers/models/classification.py\n","## Regression Modelss : https://github.com/pycaret/pycaret/blob/master/pycaret/containers/models/regression.py\n","## scikit-learn 기반 모델이면 다음과 같이 다 써볼수가 있음.\n","settings = setup(\n","                data=tr_val[FEATS], \n","                target='answerCode', \n","                train_size=0.9, \n","                categorical_features=cat_features, \n","                numeric_features=continuous_features,\n","                fold_strategy = last_fold,\n","                data_split_shuffle = False, # True가 default\n","                # fix_imbalance=True,   # True로 하면 SMOTE를 기본으로 사용\n","                # experiment_name=experiment_name,\n","                # log_experiment=False,   # compare_model 할 때에는 하이퍼파라미터가 어떻게 튜닝되었는지 볼 수 있어서 유용하다고 한다.\n","                )\n","\n","# from ngboost import NGBClassifier\n","# ngc = NGBClassifier()\n","# ngboost = create_model(ngc)\n","\n","model_name_list=['lightgbm']   # lightgbm, gbc, lda, catboost, et, ada, rf, lr, nb, knn, qda, dt, xgboost, ngboost\n","ensemble_method = 'blend_models'  # 'blend_models', 'stack_models' # (Classifier : 'stack_models'), (Regression : 'create_stacknet')\n","meta_model_for_stack = 'xgboost' # If using stack_models\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["is_all_data : False\n","valid_only_LB : test\n","train_except_holdout : False\n","train_only_LB : problem\n","train_with_finalrow : False\n","pre_split_beforehand : False\n"]},{"data":{"text/plain":["Index(['userID', 'assessmentItemID', 'testId', 'answerCode', 'Timestamp',\n","       'KnowledgeTag', 'testId_mean', 'testId_sum', 'assessmentItemID_mean',\n","       'assessmentItemID_sum', 'tag_mean', 'tag_sum', 'problem_number',\n","       'origin_problem_order', 'user_total_correct_cnt', 'user_total_ans_cnt',\n","       'user_total_acc', 'total_num_prob_in_test', 'nth_test',\n","       'user_test_ans_cnt', 'user_test_correct_cnt_per_test',\n","       'user_acc_per_test', 'int_KnowledgeTag', 'test_total_id',\n","       'answerCode_shift1'],\n","      dtype='object')"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["print(f'is_all_data : {is_all_data}')\n","print(f'valid_only_LB : {valid_only_LB}')\n","print(f'train_except_holdout : {train_except_holdout}')\n","print(f'train_only_LB : {train_only_LB}')\n","print(f'train_with_finalrow : {train_with_finalrow}')\n","print(f'pre_split_beforehand : {pre_split_beforehand}')\n","# print(f'df.columns : {df.columns}') \n","df.columns  # FEATS 참고용"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def ensemble_automl(\n","                    tr_val,\n","                    holdout,\n","                    FEATS,\n","                    cat_features=[],\n","                    continuous_features=[],\n","                    seed=seed,\n","                    model_name_list = model_name_list,\n","                    ensemble_method = ensemble_method,\n","                    meta_model_for_stack = meta_model_for_stack,\n","                    settings = settings,\n","                    ):\n","\n","    # tr_val, holdout = datasets\n","    random.seed(seed)\n","    # fold_strategy = KFold(n_splits=10, shuffle=True, random_state=42)\n","    settings = settings\n","\n","    # train_size만큼을 가지고 선언된 model_name_list 모델들을 학습을 함\n","    # create_model(name, sort='AUC', cross_validation = True)\n","    models_before_tune = [\n","                            create_model(\n","                                name, \n","                                fold=fold,\n","                                cross_validation = True,\n","                                # cross_validation = False,\n","                                ) \n","                            for name in model_name_list]\n","    # 앞서만든 모델들을 train_size만큼 가지고 튜닝함 (n_iter만큼 AutoML)\n","    # models_after_tune = [tune_model(model, optimize = 'AUC', fold = 10, n_iter = 10) for model in models_before_tune]\n","    models_after_tune = [tune_and_save_pycaret_model(name, model, optimize = 'AUC', fold = fold, n_iter = 10) for name, model in zip(model_name_list, models_before_tune)]\n","\n","\n","    # 튜닝된 모델들을 train_size만큼 가지고 앙상블\n","    if len(models_after_tune)<=1:\n","        ensembled = models_after_tune[0]\n","    else:\n","        ensembled = ensemble_and_save_pycaret_model(model_name_list,estimator_list=models_after_tune,fold = fold,optimize = 'AUC',method = method,meta_model = meta_model_for_stack,mode = ensemble_method)\n","    # elif ensemble_method=='blend_models':\n","    #     ensembled = blend_models(estimator_list = models_after_tune, fold = 10, method = 'soft', optimize = 'AUC')\n","    # elif ensemble_method=='stack_models':\n","    #     ensembled = stack_models(estimator_list = models_after_tune, meta_model = meta_model_for_stack, fold = 10, optimize = 'AUC')\n","\n","    # 마지막 학습(Finalize)\n","    # 앞서 앙상블된 모델을 => setup으로 나눠져 쓰지않았던 valid까지 포함된 100퍼센트를 사용하여 fitting함\n","    final_model = finalize_and_save_pycaret_model(model_name_list,ensembled)\n","\n","    metric_result = []\n","    prediction = predict_model(final_model, data=holdout[FEATS], raw_score = True)\n","    df_holdout_score = pull()\n","\n","    os.makedirs(f\"{upper_dir_of_pycaret}/holdout_score\",exist_ok=True)\n","    df_holdout_score.to_csv(f'{upper_dir_of_pycaret}/holdout_score/finalize_holdout_score.csv')\n","\n","    metric_result.append(f\"HoldOut 데이터 ACC & AUC: {check_metric(prediction['answerCode'], prediction['Label'], metric = 'Accuracy')} ,{check_metric(prediction['answerCode'], prediction['Score_1'], metric = 'AUC')}\")\n","    return final_model, metric_result"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["final model은 실험결과를 저장할 수 없습니다\n","Transformation Pipeline and Model Succesfully Saved\n","LGBMClassifier(bagging_fraction=0.5, bagging_freq=4, boosting_type='gbdt',\n","               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,\n","               importance_type='split', learning_rate=0.05, max_depth=-1,\n","               min_child_samples=61, min_child_weight=0.001, min_split_gain=0.8,\n","               n_estimators=70, n_jobs=-1, num_leaves=70, objective=None,\n","               random_state=7734, reg_alpha=0.5, reg_lambda=4, silent=True,\n","               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n","ACC & AUC :  HoldOut 데이터 ACC & AUC: 0.737 ,0.8145\n"]}],"source":["final_model, metric_result = ensemble_automl(\n","                                            tr_val,\n","                                            holdout,\n","                                            FEATS,\n","                                            cat_features,\n","                                            continuous_features,\n","                                            seed=seed,\n","                                            model_name_list=model_name_list,\n","                                            ensemble_method = ensemble_method,\n","                                            meta_model_for_stack = meta_model_for_stack,\n","                                            )\n","\n","print(final_model)\n","\n","print(f\"ACC & AUC : \",'\\n'.join(metric_result))\n","# 0.841 # 0.8124    # 0.8169"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["10it [00:20,  2.09s/it]"]},{"name":"stdout","output_type":"stream","text":["10-Fold ACC : 0.7330600000000002\n","10-Fold AUC : 0.8145300000000001\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# DL 팀에서 사용하고 있는 K-Fold CV 전략 사용, Score 비교\n","va_acc_list = []\n","va_auc_list = []\n","df_all_idxes = list(dict.fromkeys(df.userID))\n","# stratify_key = df.groupby(['userID'])['answerCode'].apply(lambda x: x.iloc[-1])\n","\n","kf = KFold(n_splits=10, shuffle=True, random_state=42)\n","fold_counter = 0\n","for train_idx, val_idx in tqdm(kf.split(df_all_idxes)):\n","    train_idx = np.array(df_all_idxes)[train_idx]\n","    val_idx = np.array(df_all_idxes)[val_idx]\n","\n","    # print(f\"=========({fold_counter})=========\")\n","    tr = df.loc[df.userID.isin(train_idx),:].copy()\n","    va = df.loc[df.userID.isin(val_idx),:].copy()\n","    # print(f'df.shape : {df.shape}')\n","    # print(f'tr.shape : {tr.shape}')\n","    # print(f'va.shape : {va.shape}')\n","\n","    # print(len(set(train_idx)),len(set(val_idx)))\n","    # print(f'df.userID : {len(set(df.userID))}')\n","    # print(f'tr.userID : {len(set(tr.userID))}')\n","    # print(f'va.userID : {len(set(va.userID))}')\n","\n","    # 예측할 데이터들은 user별 마지막 행\n","    rows_for_predict = va.groupby(['userID'], as_index = False).last().copy()\n","    # print(f'rows_for_predict : {rows_for_predict.shape}')\n","    va_pred = predict_model(final_model, data=rows_for_predict[FEATS], raw_score = True)\n","    va_acc = check_metric(va_pred['answerCode'], va_pred['Label'], metric = 'Accuracy')\n","    va_acc_list.append(va_acc)\n","\n","    va_auc = check_metric(va_pred['answerCode'], va_pred['Score_1'], metric = 'AUC')\n","    va_auc_list.append(va_auc)\n","    fold_counter+=1\n","\n","baseline_10fold_acc = np.mean(va_acc_list)\n","baseline_10fold_auc = np.mean(va_auc_list)\n","print(f'10-Fold ACC : {baseline_10fold_acc}')\n","print(f'10-Fold AUC : {baseline_10fold_auc}')\n","\n","# 0.83359   # 0.8379    # 0.8145"]},{"cell_type":"markdown","metadata":{"id":"3TLSfyOuFGR0"},"source":["\n","\n","# Inference"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# 이번 Experiment Arg 관리\n","use_last_fold = True\n","config_key = [\n","            'experiment_name','is_all_data','valid_only_LB','train_except_holdout','train_only_LB','train_with_finalrow',\n","            'seed','fold','method','model_name_list','ensemble_method','meta_model_for_stack',\n","            'cat_features','continuous_features','FEATS','pre_split_beforehand','use_last_fold'\n","            ]\n","config_value = [\n","                experiment_name, is_all_data, valid_only_LB, train_except_holdout, train_only_LB, train_with_finalrow, \n","                seed, fold,method, model_name_list, ensemble_method, meta_model_for_stack, \n","                cat_features, continuous_features, FEATS, pre_split_beforehand,use_last_fold]\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":523,"status":"ok","timestamp":1622561479133,"user":{"displayName":"문재훈","photoUrl":"","userId":"02977204597809886794"},"user_tz":-540},"id":"6mHls0urFGR3","outputId":"a3d84cf6-0e11-4a02-9919-dafc022ecbc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving Final Output Csv...\n","writing prediction : /opt/p4-dkt-freshtomato/output_pycaret/0614_0553/submission/0614_0553_피쳐21개사용_LGBM-mjh-21_feature_Random-10fold.csv\n","Saving Final Config Dict...\n","writing config : /opt/p4-dkt-freshtomato/output_pycaret/0614_0553/submission/0614_0553_피쳐21개사용_LGBM-mjh-21_feature_Random-10fold_config.json\n"]}],"source":["# MAKE PREDICTION\n","prediction = predict_model(final_model, data=df_test_shift[FEATS], raw_score=True)\n","total_preds = prediction.Score_1.values\n","\n","# SAVE OUTPUT\n","prediction_name = f\"{now_time}_피쳐{len(FEATS)}개사용_{experiment_name}\" # \n","\n","output_dir = f'{upper_dir_of_pycaret}/submission'\n","os.makedirs(output_dir, exist_ok=True)    \n","write_path = os.path.join(output_dir, f\"{prediction_name}.csv\")\n","print(\"Saving Final Output Csv...\")\n","with open(write_path, 'w', encoding='utf8') as w:\n","    print(f\"writing prediction : {write_path}\")\n","    w.write(\"id,prediction\\n\")\n","    for id, p in enumerate(total_preds):\n","        w.write('{},{}\\n'.format(id,p))\n","\n","# Save Config\n","write_path = os.path.join(output_dir, f\"{prediction_name}_config.json\")\n","config_dict = {k:v for k,v in zip(config_key, config_value)}\n","print(\"Saving Final Config Dict...\")\n","## json파일 저장 ##\n","with open(write_path, \"w\") as fp:\n","    print(f\"writing config : {write_path}\")\n","    json.dump(config_dict, fp, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"337f1a600d345c50cd007a2461b073851b5ec4b77bc6c65adb33d085b42175ad"},"kernelspec":{"display_name":"Python 3.8.5 64-bit ('base': conda)","name":"python3"},"language_info":{"name":"python","version":""},"orig_nbformat":3},"nbformat":4,"nbformat_minor":0}